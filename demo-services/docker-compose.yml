version: '3.8'

services:
  # Ingest Service - File analysis and project ingestion
  ingest:
    build: .
    ports:
      - "8001:8000"
    environment:
      - SERVICE_NAME=ingest
      - LOG_LEVEL=info
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: '0.5'
        reservations:
          memory: 64M
          cpus: '0.25'
    restart: unless-stopped

  # Analyze Service (Simulated)
  analyze:
    image: python:3.11-alpine
    command: >
      sh -c "pip install fastapi uvicorn &&
             echo 'from fastapi import FastAPI; app = FastAPI(); @app.get(\"/health\"); async def health(): return {\"status\": \"healthy\", \"service\": \"analyze\"}' > main.py &&
             uvicorn main:app --host 0.0.0.0 --port 8000"
    ports:
      - "8002:8000"
    environment:
      - SERVICE_NAME=analyze
    deploy:
      resources:
        limits:
          memory: 96M
          cpus: '0.3'
    restart: unless-stopped

  # Plan Service (Simulated with Go)
  plan:
    image: golang:1.21-alpine
    command: >
      sh -c "echo 'package main
             import (
                 \"encoding/json\"
                 \"net/http\"
             )
             func health(w http.ResponseWriter, r *http.Request) {
                 json.NewEncoder(w).Encode(map[string]string{\"status\": \"healthy\", \"service\": \"plan\"})
             }
             func main() {
                 http.HandleFunc(\"/health\", health)
                 http.ListenAndServe(\":8000\", nil)
             }' > main.go &&
             go run main.go"
    ports:
      - "8003:8000"
    environment:
      - SERVICE_NAME=plan
    deploy:
      resources:
        limits:
          memory: 64M
          cpus: '0.2'
    restart: unless-stopped

  # Redis for inter-service communication
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    command: redis-server --maxmemory 64mb --maxmemory-policy allkeys-lru
    deploy:
      resources:
        limits:
          memory: 64M
          cpus: '0.1'
    restart: unless-stopped

  # Nginx as API Gateway
  gateway:
    image: nginx:alpine
    ports:
      - "8080:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - ingest
      - analyze
      - plan
    deploy:
      resources:
        limits:
          memory: 32M
          cpus: '0.1'
    restart: unless-stopped

  # Monitoring (Lightweight)
  monitor:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=1h'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: '0.2'
    restart: unless-stopped

networks:
  default:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

# Total resource usage:
# Memory: ~576MB (vs 2-4GB monolithic)
# CPU: ~1.45 cores max
# Startup: ~10-15 seconds
# Services: 6 lightweight containers