Of course, Silva. Let's build **Legion**.

This is a comprehensive, step-by-step guide to building your vision. We'll break it down into phases, modules, and concrete code.

### Project Philosophy
Legion is not a monolith. It's an **orchestrator** for a swarm of single-purpose, lightweight AI agents (micro-agents). They are spawned on demand, perform a discrete task, report back, and are destroyed. This ensures efficiency and scalability.

---

### Architecture Overview

1.  **Core (`legion_core`)**: The brain. Manages the message bus, agent registry, and journaling.
2.  **Context Engine (`context_engine`)**: The memory. Understands project-wide context.
3.  **Agents (`/agents`)**: The soldiers. Pluggable Python classes for specific tasks.
4.  **Model Manager (`model_manager`)**: The arsenal. Abstracts LLM calls between local/cloud models.
5.  **Dashboard (`dashboard/)`**: The command center. A real-time UI for monitoring.
6.  **CLI (`cli.py`)**: The entry point.

```
legion/
├── __init__.py
├── cli.py                 # Command Line Interface
├── legion_core.py         # Core Orchestrator Class
├── model_manager.py       # Unified LLM API (Ollama, OpenAI, etc.)
├── context_engine.py      # Project-wide context management
├── journal.py             # Centralized logging & narration
├── config/
│   ├── default.yaml       # Default settings
│   └── user_profile.yaml  # Developer persona & preferences
├── agents/                # Directory for all micro-agents
│   ├── __init__.py
│   ├── base_agent.py
│   ├── completion_agent.py
│   ├── context_agent.py
│   ├── refactor_agent.py
│   └── narrator_agent.py  # More agents go here...
└── dashboard/
    ├── app.py             # Flask/Starlette server
    ├── static/
    └── templates/
```

---

### Phase 1: Building the Foundation

#### 1.1. The Core (`legion_core.py`)
This is the master controller.

```python
# legion/legion_core.py
import importlib
import asyncio
from pathlib import Path
from typing import Dict, Any, List
from .journal import Journal
from .context_engine import ContextEngine
from .model_manager import ModelManager

class LegionCore:
    def __init__(self, project_root: str, config: Dict[str, Any]):
        self.project_root = Path(project_root)
        self.config = config
        self.journal = Journal(self.config)
        self.context_engine = ContextEngine(self.project_root, self.journal)
        self.model_manager = ModelManager(self.config['models'])
        self.active_agents: Dict[str, Any] = {}
        
        # Load user persona for style adaptation
        self.user_profile = self._load_user_profile()
        
        self.journal.log_system("Legion core initialized")

    def _load_user_profile(self):
        profile_path = self.project_root / '.legion' / 'user_profile.yaml'
        if profile_path.exists():
            # Load and return YAML data
            pass
        return {"coding_style": "concise", "language": "en"} # Defaults

    async def execute_task(self, task_type: str, initial_context: Dict[str, Any]):
        """Main entry point: executes a task by deploying an agent chain."""
        self.journal.log_task_received(task_type, initial_context)
        
        # 1. Enrich context using the ContextEngine
        enriched_context = await self.context_engine.enrich_context(initial_context)
        
        # 2. Determine the right agent chain for the task
        agent_chain = self._orchestrate_agents(task_type, enriched_context)
        
        # 3. Execute the agent chain sequentially
        final_result = None
        for agent_name in agent_chain:
            agent_instance = self._spawn_agent(agent_name, enriched_context)
            result = await agent_instance.execute()
            enriched_context.update(result) # Pass result to next agent
            final_result = result
            
            # Agent "goes to sleep" (is dereferenced) after execution
            self.active_agents.pop(agent_name, None)
        
        # 4. Finalize (e.g., offer rollback, log completion)
        self.journal.log_task_complete(task_type, final_result)
        return final_result

    def _orchestrate_agents(self, task_type: str, context: Dict) -> List[str]:
        """Simple orchestrator logic. This can become very sophisticated."""
        agent_chains = {
            'complete_code': ['ContextAgent', 'CompletionAgent'],
            'refactor_code': ['ContextAgent', 'RefactorAgent', 'TestGenAgent', 'ReviewAgent'],
            'explain_code': ['ContextAgent', 'NarratorAgent'],
            'find_bug': ['ContextAgent', 'DebugAgent', 'ReviewAgent']
        }
        return agent_chains.get(task_type, ['ContextAgent'])

    def _spawn_agent(self, agent_name: str, context: Dict):
        """Dynamically loads and initializes an agent."""
        try:
            module = importlib.import_module(f'legion.agents.{agent_name.lower()}')
            agent_class = getattr(module, agent_name)
            # Inject dependencies into the agent
            agent_instance = agent_class(
                context=context,
                journal=self.journal,
                model_manager=self.model_manager,
                user_profile=self.user_profile
            )
            self.active_agents[agent_name] = agent_instance
            self.journal.log_system(f"Spawned agent: {agent_name}")
            return agent_instance
        except (ImportError, AttributeError) as e:
            self.journal.log_error(f"Failed to spawn agent {agent_name}: {e}")
            raise
```

#### 1.2. The Model Manager (`model_manager.py`)
Unified interface for all LLMs. **Key for offline-first.**

```python
# legion/model_manager.py
import openai
from ollama import Client as OllamaClient
import logging

class ModelManager:
    def __init__(self, model_config: Dict[str, Any]):
        self.config = model_config
        self.ollama_client = OllamaClient(host=model_config['ollama']['host']) if model_config['ollama']['enabled'] else None
        # Configure other clients (OpenAI, OpenRouter) similarly
        self.logger = logging.getLogger(__name__)

    async def generate(self, prompt: str, **kwargs) -> str:
        """Generates a completion using the preferred model with fallbacks."""
        models_to_try = [
            self._try_ollama,
            self._try_openai,   # Paid API
            self._try_openrouter # Free-tier API
        ]
        
        for model_func in models_to_try:
            try:
                response = await model_func(prompt, **kwargs)
                if response:
                    return response
            except Exception as e:
                self.logger.warning(f"Model {model_func.__name__} failed: {e}")
                continue
        raise Exception("All model providers failed.")

    async def _try_ollama(self, prompt: str, **kwargs) -> str:
        if not self.ollama_client:
            return None
        response = self.ollama_client.generate(model=self.config['ollama']['model'], prompt=prompt, options={"temperature": 0.1})
        return response['response']

    async def _try_openai(self, prompt: str, **kwargs) -> str:
        # ... Similar implementation using OpenAI client
        pass

    async def _try_openrouter(self, prompt: str, **kwargs) -> str:
        # ... Similar implementation using OpenRouter API
        pass
```

#### 1.3. The Journal & Narrator (`journal.py`)
The central nervous system for logging and voice.

```python
# legion/journal.py
import json
import datetime
from pathlib import Path
from typing import Dict, Any
# import TTS library, e.g., TTS (Coqui AI)

class Journal:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.log_file = Path(config['journal_path']) / 'log.jsonl'
        self.log_file.parent.mkdir(exist_ok=True)
        # self.tts_engine = TTS() if config['narration']['enabled'] else None

    def log(self, event_type: str, data: Dict[str, Any], agent_name: str = "system"):
        """Logs an event to the journal and triggers narration."""
        entry = {
            "timestamp": datetime.datetime.utcnow().isoformat(),
            "agent": agent_name,
            "type": event_type,
            "data": data
        }
        # Append to structured log file
        with open(self.log_file, 'a') as f:
            f.write(json.dumps(entry) + '\n')
        
        # Broadcast for dashboard (could use a websocket here)
        self._broadcast_for_dashboard(entry)
        
        # Trigger narration for specific important events
        if event_type in ['task_complete', 'error', 'suggestion']:
            self._narrate_event(entry)

    def _narrate_event(self, entry: Dict):
        if not self.config['narration']['enabled']:
            return
        narration_map = {
            'task_complete': f"Task completed by {entry['agent']}.",
            'error': f"Error encountered: {entry['data'].get('message')}",
            'suggestion': f"I have a suggestion for you."
        }
        text = narration_map.get(entry['type'], None)
        if text and self.tts_engine:
            # Run in a thread to avoid blocking
            # self.tts_engine.tts_to_file(text=text, file_path="narration_output.wav")
            # Play the audio file
            pass
```

---

### Phase 2: Building the Micro-Agents

#### 2.1. Base Agent Class (`agents/base_agent.py`)
The blueprint for all agents.

```python
# legion/agents/base_agent.py
from abc import ABC, abstractmethod
from typing import Dict, Any

class BaseAgent(ABC):
    def __init__(self, context: Dict, journal, model_manager, user_profile: Dict):
        self.context = context
        self.journal = journal
        self.model_manager = model_manager
        self.user_profile = user_profile
        self.agent_name = self.__class__.__name__

    @abstractmethod
    async def execute(self) -> Dict[str, Any]:
        """The main method every agent must implement. Returns a result dict."""
        pass

    def _build_prompt(self, template: str, **kwargs) -> str:
        """Helper method to build prompts, injecting user profile and context."""
        # Inject user style and language into the prompt
        prompt_vars = {
            'user_style': self.user_profile.get('coding_style', ''),
            'language': self.user_profile.get('language', 'en'),
            **self.context,
            **kwargs
        }
        return template.format(**prompt_vars)
```

#### 2.2. Context Agent (`agents/context_agent.py`)
The most important agent. The scout that gathers intelligence.

```python
# legion/agents/context_agent.py
from .base_agent import BaseAgent

class ContextAgent(BaseAgent):
    async def execute(self) -> Dict[str, Any]:
        self.journal.log("info", {"message": "Gathering project context..."}, self.agent_name)
        
        # This is a simplified example. The real ContextEngine would do RAG.
        current_file = self.context.get('file_path', '')
        current_code = self.context.get('code', '')
        
        # Simulate getting relevant context
        related_context = {
            'related_files': ['/utils/helpers.py', '/config/settings.py'],
            'related_code_snippets': [
                'def helper_function():\n    # This is from helpers.py\n    pass'
            ],
            'project_structure': "This is a Python project with a Flask backend and React frontend."
        }
        
        result = {"enriched_context": related_context}
        self.journal.log("context_updated", result, self.agent_name)
        return result
```

#### 2.3. Completion Agent (`agents/completion_agent.py`)
The workhorse.

```python
# legion/agents/completion_agent.py
from .base_agent import BaseAgent

class CompletionAgent(BaseAgent):
    async def execute(self) -> Dict[str, Any]:
        self.journal.log("info", {"message": "Generating code completion..."}, self.agent_name)
        
        prompt_template = """
        Language: {language}
        Developer's Coding Style: {user_style}
        
        Project Overview:
        {project_structure}
        
        Relevant Code from other files:
        {related_code_snippets}
        
        Here is the code I'm currently working on in the file `{file_path}`:
        ```{language}
        {code}
        ```
        
        Please generate a concise and context-aware completion. Only output the code.
        """
        
        prompt = self._build_prompt(prompt_template, **self.context['enriched_context'])
        completion = await self.model_manager.generate(prompt)
        
        result = {"suggestion": completion, "original_code": self.context['code']}
        self.journal.log("suggestion_generated", result, self.agent_name)
        return result
```

---

### Phase 3: Integration & Execution

#### 3.1. Command Line Interface (`cli.py`)
How the user interacts with Legion.

```python
# legion/cli.py
import argparse
import asyncio
from pathlib import Path
from .legion_core import LegionCore
import yaml

def load_config():
    """Loads configuration from default and user files."""
    default_config = Path(__file__).parent / 'config' / 'default.yaml'
    user_config = Path.home() / '.legion' / 'config.yaml'
    
    with open(default_config, 'r') as f:
        config = yaml.safe_load(f)
    if user_config.exists():
        with open(user_config, 'r') as f:
            user_config_data = yaml.safe_load(f)
            config.update(user_config_data) # User config overrides defaults
    return config

async def main():
    parser = argparse.ArgumentParser(description='Legion: AI Code Agent Swarm')
    parser.add_argument('task', help='The task to perform (e.g., complete_code, refactor_code)')
    parser.add_argument('--file', required=True, help='File to operate on')
    parser.add_argument('--code', help='Current code snippet (if not reading from file)')
    args = parser.parse_args()
    
    config = load_config()
    project_root = Path.cwd() # Assume CWD is the project root
    
    # Initialize Legion
    legion = LegionCore(project_root, config)
    
    # Prepare initial context
    context = {
        'file_path': args.file,
        'code': args.code or Path(args.file).read_text(),
        'task_description': args.task
    }
    
    # Execute the task
    result = await legion.execute_task(args.task, context)
    print(f"Result: {result}")

if __name__ == '__main__':
    asyncio.run(main())
```
**Usage:**
```bash
legion complete_code --file ./src/app.py --code "def get_user(id):"
```

#### 3.2. Configuration (`config/default.yaml`)
```yaml
# legion/config/default.yaml
models:
  ollama:
    enabled: true
    host: "http://localhost:11434"
    model: "codellama:7b"
  openai:
    enabled: false
    model: "gpt-4"
    api_key: ""
  openrouter:
    enabled: false
    model: "anthropic/claude-3-haiku"
    api_key: ""
journal:
  enabled: true
  path: "./.legion/logs"
narration:
  enabled: false
  language: "en" # For TTS
```

---

### Phase 4: Next Steps & Advanced Features

1.  **Real Dashboard:** Build a simple web server (`dashboard/app.py`) using Flask/Starlette that uses Server-Sent Events (SSE) to stream journal entries to a web UI in real-time.
2.  **Rollback Mechanism:** Integrate with Git. Before any agent makes a change, `git stash` or commit to a temporary branch. The journal stores the commit hash, allowing for one-click undo.
3.  **Advanced Context Engine:** Implement a proper RAG pipeline using a local vector database like `ChromaDB` or `LanceDB` to index the entire codebase for truly semantic search.
4.  **Agent Plugins:** Create a standard `pyproject.toml` entry point so anyone can build and install their own agents for Legion. `legion install agent-legion-linter`.

This is the complete blueprint, Silva. Start with `Phase 1` and `Phase 2`. Build the core, the model manager, and two simple agents (`Context` and `Completion`). The rest will flow from there.

You are building the future of developer tooling. Let's code.